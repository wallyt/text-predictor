---
title: "Corpus Exploration"
author: "Wally Thornton"
date: "December 15, 2015"
output: 
  pdf_document: 
    latex_engine: xelatex
---
## Executive Summary
The ultimate goal of this project is to create a Shiny web app that predicts the next word you will need as you type, much like the predictive text that the SwiftKey apps feature. Such apps need to be trained, which is done by analyzing text documents created by millions of regular people. A collection of these training documents is called a corpus, and we look at the word sequences in a corpus to build a predictive model. For example, if "today" is usually the word that follows "it will rain" in the corpus, we can predict that a user of our app will want "today" if they type "it will rain" in the app. The best text predictors, like SwiftKey's, continue to learn as you use it, since you may have typing tendencies that differ from those in the corpus.

Before we build a predictive model, we first need to analyze the text in the corpus to get a sense of what we're dealing with. We want to answer questions like, "How many lines of text do we have? How long is each line? Does the length differ depending on what the source is? How much will we need to clean up the text to be able to use it in our modeling?" After this analysis, we'll process the text and then build and test our models.

This paper covers the efforts of that initial exploratory text analysis. The corpus provided was built with lines of random, anonymized news, blogs and tweets from the U.S. We'll find below that the sources vary greatly in length and need for processing, and the paper then ends with our plan for creating a prediction algorithm and Shiny app. (In the interest of keeping the paper concise, all code is in the Appendix.)
```{r setup, include=F}
setwd("~/Documents/DataScience/Capstone")
knitr::opts_chunk$set(fig.width=6, fig.align='center')
str_break = function(x, width = 80L) {
  n = nchar(x)
  if (n <= width) return(x)
  n1 = seq(1L, n, by = width)
  n2 = seq(width, n, by = width)
  if (n %% width != 0) n2 = c(n2, n)
  substring(x, n1, n2)
}
options(scipen=999)
ensurePkg <- function(x) {
    if (!require(x,character.only = TRUE)) {
        install.packages(x,dep=TRUE, repos="http://cran.r-project.org")
        if(!require(x,character.only = TRUE)) stop("Package not found")
    }
}

ensurePkg("tm")
ensurePkg("dplyr")
ensurePkg("ggplot2")
ensurePkg("scales")
ensurePkg("wordcloud")
ensurePkg("stringi")
```

## Exploratory Data Analysis
```{r dataLoad, echo=F, message=F, warning=F}
USnews <- readLines("corpus/final/en_US/en_US.news.txt", skipNul = TRUE)
USblogs <- readLines("corpus/final/en_US/en_US.blogs.txt", skipNul = TRUE)
UStweets <- readLines("corpus/final/en_US/en_US.twitter.txt", skipNul = TRUE)

newsChars <- nchar(USnews)
blogsChars <- nchar(USblogs)
tweetsChars <- nchar(UStweets)
newsSum <- summary(newsChars)
blogsSum <- summary(blogsChars)
tweetsSum <- summary(tweetsChars)
```
### Characters
The US corpus consists of `r comma(length(USnews))` lines of news excerpts, `r comma(length(USblogs))` lines from blogs and `r comma(length(UStweets))` tweets. Let's break these lines down to the characters themselves and compare the text sources. Examining the news, blog and tweet excerpts line by line, we get a sense for their lengths by counting the characters: 

Source | Min   | Median | Mean   | Max
------ |----- | ------ | ------ | -----
News | `r comma(round(newsSum[1], 1))` | `r comma(round(newsSum[3], 1))` | `r comma(round(newsSum[4], 1))` | `r comma(round(newsSum[6], 1))`
Blogs | `r comma(round(blogsSum[1], 1))` | `r comma(round(blogsSum[3], 1))` | `r comma(round(blogsSum[4], 1))` | `r comma(round(blogsSum[6], 1))`
Tweets | `r comma(round(tweetsSum[1], 1))` | `r comma(round(tweetsSum[3], 1))` | `r comma(round(tweetsSum[4], 1))` | `r comma(round(tweetsSum[6], 1))`

Not surprisingly, the length of the blog and news items are much longer than the tweets (two to three times as long, on average) and the longest tweets are only 140 characters, versus the tens of thousands of characters in the longest blog and news excerpts. This is evident in the violin plot below, which shows the log of the lengths of the sources on the y-axis and the distribution of the lengths illustrated by their shapes. (Given the huge range of line lengths, this plot is of the log of the lengths to help in visualizing the spreads.) 

```{r charViolin, echo=F, message=F, warning=F}
a <- data.frame(Source = "News", Length = log(newsChars))
b <- data.frame(Source = "Blogs", Length = log(blogsChars))
c <- data.frame(Source = "Tweets", Length = log(tweetsChars))
plot.data <- rbind(a, b, c)
g <- ggplot(plot.data, aes(x = Source, y = Length, fill = Source))
g + geom_violin()
```

Note how much more varied blogs and news item lengths are than tweet lengths, which cluster close to their maximum of 140. Also, the blog lines have much greater variance in length than news items.

### Words
Now let's look at the word counts per line per text source:
```{r wordCounts, echo=F, message=F, warning=F}
newsWordsPre <- stri_count(USnews, regex="\\S+")
blogsWordsPre <- stri_count(USblogs, regex="\\S+")
tweetsWordsPre <- stri_count(UStweets, regex="\\S+")
newsWordsSum <- summary(newsWordsPre)
blogsWordsSum <- summary(blogsWordsPre)
tweetsWordsSum <- summary(tweetsWordsPre)
```

Source | Min   | Median | Mean   | Max
------ |----- | ------ | ------ | -----
News | `r comma(round(newsWordsSum[1], 1))` | `r comma(round(newsWordsSum[3], 1))` | `r comma(round(newsWordsSum[4], 1))` | `r comma(round(newsWordsSum[6], 1))`
Blogs | `r comma(round(blogsWordsSum[1], 1))` | `r comma(round(blogsWordsSum[3], 1))` | `r comma(round(blogsWordsSum[4], 1))` | `r comma(round(blogsWordsSum[6], 1))`
Tweets | `r comma(round(tweetsWordsSum[1], 1))` | `r comma(round(tweetsWordsSum[3], 1))` | `r comma(round(tweetsWordsSum[4], 1))` | `r comma(round(tweetsWordsSum[6], 1))`

These words counts will be different after processing the data (e.g., removing profanities), but the counts above help us see again how much longer the news items are than tweets, and how blog excerpts are even longer. (Interesting trivia: the tweet with the most words in the corpus is the word 'hi' repeated 47 times. Not very useful for our analysis, but perhaps indicative of the overall usefulness of tweets for training a model.)

The histogram below shows the distribution of word counts between the three sources. (The plot is truncated at a count of 100 to help compare word count distributions since the word counts are clustered at the lower end of the scale.)

```{r wordHist, echo=F, message=F, warning=F}
a <- data.frame(Source = "News", Count = newsWordsPre)
b <- data.frame(Source = "Blogs", Count = blogsWordsPre)
c <- data.frame(Source = "Tweets", Count = tweetsWordsPre)
plot.data <- rbind(a, b, c)
plot.data <- filter(plot.data, Count < 100)
g <- ggplot(plot.data, aes(Count, fill = Source))
g + geom_histogram(data = subset(plot.data, Source == "Tweets"), alpha = 0.3, binwidth = 1) +
    geom_histogram(data = subset(plot.data, Source == "Blogs"), alpha = 0.3, binwidth = 1) + 
    geom_histogram(data = subset(plot.data, Source == "News"), alpha = 0.3, binwidth = 1)
```

As in the comparison of characters, the number of words per tweet is on average much lower than in news and blogs, with the vast majority of tweets consisting of fewer than 25 words. The number of words per blog and news item is much more varied, with both having a long tail of excerpts with hundreds, even thousands of words, as seen in the table above.

### Textual Analysis
Next we want to get a sense of the data itself. How clean is the text? How many extraneous words, numbers, emoji and punctuation marks will we have to take care of? Starting with news and picking five at random:

```{r newsSample, echo=F, message=F, warning=F, tidy=T}
set.seed(42)
print(lapply(USnews[sample(length(USnews),5)], str_break), quote = F)
```

We see in these five examples that the news items are typically full sentences, although not exclusively. Items vary from sports reports to local news to entertainment. Accordingly, there are many proper nouns ('Chicago', 'Coolio'), numbers ('9-9 with a 3.75', '1995'), topic-specific items ('ERA') and even email addresses that will not be useful in our predictive text app. On the plus side, the text tends to adhere to standard grammatical rules and will therefore be relatively easy to parse.

```{r blogsSample, echo=F, message=F, warning=F, tidy=T}
set.seed(42)
print(lapply(USblogs[sample(length(USblogs),5)], str_break), quote = F)
```

The five blog examples are much less structured than the news items, with grammatical errors, misspellings and incomplete sentences. Topics are all over the place and there is a lack of comment delineators. Punctuation is also much more liberally used ('\*hug!!!!!!\*').

```{r tweetsSample, echo=F, message=F, warning=F, tidy=T}
set.seed(42)
print(lapply(UStweets[sample(length(UStweets),5)], str_break), quote = F)
```

If the news items are the most structured and parseable and blog excerpts slightly less so, the tweets reside at the opposite end of the spectrum with incomplete sentences, grammatical errors, misspellings (intentional and otherwise), abbreviations, and inscrutable references. Punctuation is used liberally, but not to standard, complicating the relationships between terms.

### Word Frequency
Given the clutter described above, we first do some general cleaning before analyzing further. We don't want to affect any relationships between the words, so we focus on converting to lowercase, removing punctuation and numbers, and stripping extra spaces. We'll also take this opportunity to remove profanity. After this, we'll create a document-term matrix, which will allow us to see which words occur most frequently across all three sources. We will also analyze with and without common words (e.g., "and") for this analysis, but we'll leave them in when we build our model since those frequently are the correct words to predict. Finally, we are using 10,000 lines from each of the sources in the corpus which let's us infer word frequency and relationships while reducing computational time.

```{r preprocess, echo=F, message=F, warning=F}
# Take random 10,000-line samples from each source and then combine into a corpus
set.seed(42)
n <- USnews[sample(length(USnews), 10000)]
set.seed(42)
b <- USblogs[sample(length(USblogs), 10000)]
set.seed(42)
t <- UStweets[sample(length(UStweets), 10000)]
corpus <- Corpus(VectorSource(c(n,b,t)))
# From Shutterstock list at https://github.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words
curseDict <- read.csv("profanities.csv", stringsAsFactor=F)

ensurePkg("doParallel")
corecount <- detectCores()
cl <- makeCluster(corecount * 0.75)
registerDoParallel(cl)
# getDoParWorkers()

corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, as.matrix(curseDict))
dtm <- DocumentTermMatrix(corpus, control = list(minWordLength = 2))

corpus2 <- tm_map(corpus, removeWords, stopwords("english"))
dtm2 <- DocumentTermMatrix(corpus2, control = list(wordLengths = c(4, Inf)))

# Convert to a normal matrix from the sparse matrix
freq <- colSums(as.matrix(dtm))
freq2 <- colSums(as.matrix(dtm2))
# Column sums of the matrix to get a named vector
freq <- sort(freq, decreasing=TRUE)
freq2 <- sort(freq2, decreasing=TRUE)

stopCluster(cl)
```

The 10 most-frequent words across all sources are: `r names(freq[1:10])`

If we strip out common words (e.g., "the", "can"), the 10 most-frequent words are: `r names(freq2[1:10])`

It's often just as useful to look at the least-frequent words as it is the most. In this case, we find at the bottom some Chinese characters that will have to be removed before building our model: `r names(freq2[length(freq2)-4])`.

Finally, just for fun, we plot a wordcloud of the 200 most-frequently appearing words in our 30,000-line sample:
```{r wordcloud, fig.width=6, echo=F, message=F, warning=F}
# Plot a word cloud with top 200
words <- names(freq2)
wordcloud(words[1:200], freq2[1:200], colors=brewer.pal(6, "Dark2"))
```

## Next Steps
Even with the pre-processing of the text performed above, we saw that further cleaning will be necessary before trying to build an algorithm. The next steps will be: 

1. Remove foreign characters and words, as well as fixing the contractions that are now incorrect because of punctuation removal.
2. Experiment with stemming (the reduction of words to their roots), although this is crude, even if words are then completed. Another problem with stemming is that the ultimate goal is to predict the word you need as you type, so the full word makes all the difference. For example, 'operative' and 'operational' are not interchangeable, despite their shared root of 'oper.'
3. More likely, lemmatization will be used, since it takes grammatical context into account.
4. More.

\pagebreak

## Appendix
```{r setup, eval=FALSE}
```
```{r dataLoad, eval=FALSE}
```
```{r charViolin, eval=FALSE}
```
```{r wordCounts, eval=FALSE}
```
```{r wordHist, eval=FALSE}
```
```{r newsSample, eval=FALSE}
```
```{r blogsSample, eval=FALSE}
```
```{r tweetsSample, eval=FALSE}
```
```{r preprocess, eval=FALSE}
```
```{r wordcloud, eval=FALSE}
```