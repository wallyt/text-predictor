---
title: "Corpus Exploration"
author: "Wally Thornton"
date: "December 15, 2015"
output: html_document
---
## Executive Summary

```{r setup, include=F}
knitr::opts_chunk$set(fig.width=7, fig.align='center')
options(scipen=999)
ensurePkg <- function(x) {
    if (!require(x,character.only = TRUE)) {
        install.packages(x,dep=TRUE, repos="http://cran.r-project.org")
        if(!require(x,character.only = TRUE)) stop("Package not found")
    }
}

ensurePkg("tm")
ensurePkg("dplyr")
ensurePkg("ggplot2")
ensurePkg("scales")
ensurePkg("wordcloud")
ensurePkg("stringi")
# help.search(keyword = "character", package = "base")
```

## Exploratory Data Analysis

```{r dataLoad, echo=F, message=F, warning=F}
#USnewsExcerpt <- readLines("corpus/final/en_US/en_US.news.txt", skipNul = TRUE, n=10000)
USnews <- readLines("corpus/final/en_US/en_US.news.txt", skipNul = TRUE)
USblogs <- readLines("corpus/final/en_US/en_US.blogs.txt", skipNul = TRUE)
UStweets <- readLines("corpus/final/en_US/en_US.twitter.txt", skipNul = TRUE)
corpus <- Corpus(DirSource("corpus/final/en_US"))
#head(corpus[[1]]$content)
#length(content(corpus[[1]]))

newsChars <- nchar(USnews)
blogsChars <- nchar(USblogs)
tweetsChars <- nchar(UStweets)
newsSum <- summary(newsChars)
blogsSum <- summary(blogsChars)
tweetsSum <- summary(tweetsChars)
```

The US corpus consists of `r comma(length(USnews))` lines of news excerpts, `r comma(length(USblogs))` lines from blogs and `r comma(length(UStweets))` tweets. Let's break these lines down to the characters themselves and compare the text sources. Examining the news, blog and tweet excerpts line by line, we get a sense for their lengths by counting the characters: 

Source | Min   | Median | Mean   | Max
------ |----- | ------ | ------ | -----
News | `r comma(round(newsSum[1], 1))` | `r comma(round(newsSum[3], 1))` | `r comma(round(newsSum[4], 1))` | `r comma(round(newsSum[6], 1))`
Blogs | `r comma(round(blogsSum[1], 1))` | `r comma(round(blogsSum[3], 1))` | `r comma(round(blogsSum[4], 1))` | `r comma(round(blogsSum[6], 1))`
Tweets | `r comma(round(tweetsSum[1], 1))` | `r comma(round(tweetsSum[3], 1))` | `r comma(round(tweetsSum[4], 1))` | `r comma(round(tweetsSum[6], 1))`

Not surprisingly, the length of the blog and news items are much longer than the tweets (two to three times as long, on average) and the longest tweets are only 140 characters, versus the tens of thousands of characters in the longest blog and news excerpts. This is evident in the boxplots below, which show how much more varied blogs and news item lengths are than tweets. Note that the blog lines have much greater variance in length than even news items. (Given the huge range of line lengths, this plot is of the log of the lengths to visualize the spread.)
```{r charBox, echo=F, message=F, warning=F}
a <- data.frame(Source = "News", Length = log(newsChars))
b <- data.frame(Source = "Blogs", Length = log(blogsChars))
c <- data.frame(Source = "Tweets", Length = log(tweetsChars))
plot.data <- rbind(a, b, c)
g <- ggplot(plot.data, aes(x = Source, y = Length, fill = Source))
g + geom_boxplot()
```

Now let's look at the word counts per line per text source, using the raw, un-processed data:
```{r wordCountsPre, echo=F, message=F, warning=F}
newsWordsPre <- stri_count(USnews, regex="\\S+")
blogsWordsPre <- stri_count(USblogs, regex="\\S+")
tweetsWordsPre <- stri_count(UStweets, regex="\\S+")
```

Source | Min   | Median | Mean   | Max
------ |----- | ------ | ------ | -----
News | `r comma(round(newsWordsPre[1], 1))` | `r comma(round(newsWordsPre[3], 1))` | `r comma(round(newsWordsPre[4], 1))` | `r comma(round(newsWordsPre[6], 1))`
Blogs | `r comma(round(blogsWordsPre[1], 1))` | `r comma(round(blogsWordsPre[3], 1))` | `r comma(round(blogsWordsPre[4], 1))` | `r comma(round(blogsWordsPre[6], 1))`
Tweets | `r comma(round(tweetsWordsPre[1], 1))` | `r comma(round(tweetsWordsPre[3], 1))` | `r comma(round(tweetsWordsPre[4], 1))` | `r comma(round(tweetsWordsPre[6], 1))`

COMMENTARY


Before turning our attention to processing the texts, we first get a sense of the data itself. How clean is the text? How many extraneous words, numbers, emoji and punctuation marks will we have to take care of? Starting with news and picking five at random:

```{r newsSample, echo=F, message=F, warning=F}
set.seed(42)
USnews[sample(1:length(USnews),5)]
```

We see in these five examples that the news items are typically full sentences, although not exclusively. Items vary from sports reports to local news to entertainment. Accordingly, there are many proper nouns ('Chicago', 'Coolio'), numbers ('9-9 with a 3.75', '1995'), topic-specific items ('ERA') and even email addresses that will not be useful in our predictive text app. On the plus side, they tend to adhere to standard grammatical rules and will therefore be easier to parse.

```{r blogsSample, echo=F, message=F, warning=F}
set.seed(420)
USblogs[sample(1:length(USblogs),5)]
```

The five blog examples are much less structured than the news items, with grammatical errors, misspellings and incomplete sentences. Topics are all over the place and there is a lack of comment delineators. Punctuation is also much more liberally used (e.g., '\*hug!!!!!!\*').

```{r tweetsSample, echo=F, message=F, warning=F}
set.seed(42)
UStweets[sample(1:length(UStweets),5)]
```

If the news items are the most structured and parseable and blog excerpts slightly less so, the tweets reside at the opposite end of the spectrum with incomplete sentences, grammatical errors, misspellings (intentional and otherwise), abbreviations, and inscrutable references. Punctuation is used liberally, but not to standard, complicating the relationships between terms.

## Preprocessing
Given the clutter described above, we first do some general cleaning before analyzing further. We don't want to affect any relationships between the words, so we focus on converting to lowercase, removing punctuation and numbers, and stripping extra spaces. We'll also take this opportunity to remove profanity.

After this, we'll create a document-term matrix, which will allow us to see which words occur most frequently across all three sources. We don't want to remove any common words (e.g., "and") at this point because they'll be needed as both inputs and outputs of the predictive model.

```{r preprocess, echo=F, message=F, warning=F}
# From Shutterstock list at https://github.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words
curseDict <- read.csv("profanities.csv", stringsAsFactor=F)

# DigitalOcean droplet can't handle multiple cores for some reason
coreLim <- ifelse(grepl("linux", R.version$platform), ", mc.cores=1", "")

ensurePkg("doParallel")
corecount <- detectCores()
cl <- makeCluster(corecount * 0.75)
registerDoParallel(cl)
getDoParWorkers()
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, as.matrix(curseDict))
corpus <- tm_map(corpus, removeWords, stopwords("english"))

dtm <- DocumentTermMatrix(corpus, control = list(minWordLength = 1))
stopCluster(cl)

# Word count per document
rowSums(as.matrix(dtm))

```

Word count comparison post clean

Boxplot or histogram comparing line lengths in words

Frequency comparison

Frequency combined

Word pair frequencies

```{r wordcloud, message=F, warning=F}
# For the wordcloud, we'll remove the common words
corpusCloud <- tm_map(corpus, removeWords, stopwords)
dtmCloud <- DocumentTermMatrix(corpusCloud)
# Convert to a normal matrix from the sparse matrix
dtmCloud2 <- as.matrix(dtmCloud)
# Column sums of the matrix to get a named vector
frequency <- colSums(dtmCloud2)
frequency <- sort(frequency, decreasing=TRUE)

# Plot a word cloud with top 100
words <- names(frequency)
wordcloud(words[1:200], frequency[1:200], colors=brewer.pal(6, "Dark2"))

# Find associations of words occurring at least 200 times
dtmCloudHighFreq <- findFreqTerms(dtmCloud, 200, Inf)
findAssocs(dtmCloud, "love", 0.85)
```

## Plan
I don't want to stem because I want to predict the appropriate derivative of a word, which depends on the preceding words, although one option is to use stemDocument and then stemComplete (using dictCorpus<-myCorpus; dictionary=dictCorpus) to make them readable.


Preprocess
Associate
Cluster
Summarize
Categorize
http://www.r-bloggers.com/text-mining-in-r-automatic-categorization-of-wikipedia-articles/
API